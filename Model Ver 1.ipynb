{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hdx.utilities.easy_logging import setup_logging\n",
    "from hdx.hdx_configuration import Configuration\n",
    "from hdx.data.dataset import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "setup_logging()\n",
    "Configuration.create(hdx_site='prod', user_agent='A_Quick_Example', hdx_read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check if the dataset has at least 1 resource of the required file type(s).\n",
    "\n",
    "def check_type(dataset, file_types=[]):\n",
    "    temp_dataset = Dataset.read_from_hdx(dataset)\n",
    "    temp_dataset.separate_resources()\n",
    "    if (len(temp_dataset.resources) > 0):\n",
    "        if (len(file_types) > 0):\n",
    "            if (not set(temp_dataset.get_filetypes()).isdisjoint(file_types)): \n",
    "                    return True\n",
    "        else :\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if the dataset is tagged with HXL tag, not provided by HXL\n",
    "\n",
    "def check_organization(dataset):\n",
    "    if dataset.get_organization()['title'] != 'Humanitarian Exchange Language(HXL)':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#CLEANING AND GENERATING N-GRAMS\n",
    "\n",
    "def lower_cols(lst):\n",
    "    #convert data to lowercases\n",
    "    #QUESTION: will I miss anyt important information? \n",
    "    return [word.lower() for word in lst]\n",
    "\n",
    "#Question: is HXL Core Schema.csv something we can use for comparing words??\n",
    "#This method is going to take up a lot of space and time. Is it worth it? Are there any other ways to go about it? \n",
    "\n",
    "def remove_chars(lst):\n",
    "    #remove punctuation characters such as \",\", \"(\", \")\", \"\"\", \":\", \"/\", and \".\"\n",
    "    #NOTE: PRESERVES WHITE SPACE.\n",
    "    #QUESTION: any other characters we should be aware of? Is this a good idea? I'm inspecting each word individually.\n",
    "    #Any potential pitfalls? \n",
    "    cleaned = [re.sub('\\s+', ' ', mystring).strip() for mystring in lst]\n",
    "    cleaned = [re.sub(r'[[^A-Za-z0-9\\s]+]', ' ', mystr) for mystr in cleaned]\n",
    "    cleaned = [mystr.replace('_', ' ') for mystr in cleaned]\n",
    "    return cleaned\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(data_lst):\n",
    "    #remove stopwords from the data including 'the', 'and' etc. \n",
    "    wordsFiltered = []\n",
    "    for w in data_lst:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered\n",
    "\n",
    "def clean_cols(data):\n",
    "    data = lower_cols(data)\n",
    "    data = remove_chars(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download one dataset with certain type(s), read it into Dataframe, \n",
    "# add all headers, tags and dataset names to our DataFrame,\n",
    "# and delete the dataset\n",
    "\n",
    "def process_dataset_2(dataset, file_type, dataframe, download_path, index, row_limit = 10):\n",
    "    global count\n",
    "    organization = \"\"\n",
    "# Download one dataset and read it into a DataFrame \n",
    "    if (file_type == None):\n",
    "        url, path = dataset.resources[0].download(download_path)\n",
    "        pandas_dataset = pd.read_csv(path)\n",
    "    else:\n",
    "        if (file_type not in dataset.get_filetypes()):\n",
    "            return 'Error: Required file type not in dataset OR dataset does not contain any resources.'\n",
    "        try:\n",
    "            url, path = dataset.resources[dataset.get_filetypes().index(file_type)].download(download_path)\n",
    "            organization = dataset.get_organization()['title']\n",
    "            print('Resource URL %s downloaded to %s' % (url, path))\n",
    "            pandas_dataset = pd.read_csv(path, encoding='latin-1')\n",
    "            pandas_dataset = pandas_dataset.head(row_limit)\n",
    "        except:\n",
    "            return 'Unknown error.'\n",
    "     \n",
    "    #if \"HXL\" in os.path.basename(path) or \"hxl\" in os.path.basename(path):\n",
    "        #return dataset_df\n",
    "    \n",
    "# Add headers, tags and data to our DataFrame if current dataset not empty\n",
    "        if (not pandas_dataset.empty):\n",
    "            dataset_df = pandas_dataset\n",
    "            headers = list(dataset_df.columns.values)\n",
    "            headers = clean_cols(headers)\n",
    "            tags = list(dataset_df.iloc[0,:])\n",
    "            for i in range(len(headers)):\n",
    "                try:\n",
    "                    splitted = re.split('[(^\\s+)+#]', tags[i])\n",
    "                    splitted = list(filter(None, splitted))\n",
    "                    hashtag = splitted[0]\n",
    "                    attributes = splitted[1:]\n",
    "                    dic = {'Header': headers[i], 'Tag': hashtag, 'Attributes': attributes, \n",
    "                           'Data': list(dataset_df.iloc[1:, i]), \n",
    "                           'Relative Column Position': (i+1) / len(dataset_df.columns), \n",
    "                           'Dataset_name': os.path.basename(path), \n",
    "                           'Organization': organization,\n",
    "                           'Index': index}\n",
    "                    dataframe.loc[len(dataframe)] = dic\n",
    "                except:\n",
    "                    print(\"Error: different number of headers and tags\")\n",
    "            count += 1\n",
    "        os.remove(path)\n",
    "        print(\"File Removed!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Search for all datasets with HXL tags\n",
    "\n",
    "datasets_HXL = Dataset.search_in_hdx('HXL')\n",
    "len(datasets_HXL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for all headers and tags\n",
    "\n",
    "col_names = ['Header', 'Tag', 'Attributes','Data','Relative Column Position','Dataset_name', 'Organization','Index']\n",
    "headers_and_tags= pd.DataFrame(columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(150):\n",
    "    rand_dataset = np.random.randint(0, len(datasets_HXL))\n",
    "    process_dataset_2(datasets_HXL[rand_dataset], 'CSV', headers_and_tags, './datasets', count)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers_and_tags.to_excel(\"headerandtag.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers_and_tags.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#counting column names that appear the most frequently in the set:\n",
    "from collections import Counter\n",
    "counts = Counter(headers_and_tags.iloc[:, 0])\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implementing Bag Of Words Model\n",
    "#STEPS\n",
    "#1) collect all col_names that have the same underlying meaning ex: deaths/fatalities etc. (see tags) \n",
    "#2) construct another dataframe that consists of the following headers: \n",
    "#3) [Header, Word Frequency]... could be a dictionary or multi-indexed dataframe where second column is all the words that \n",
    "#appear in the data under the given col name and the value would be count of word that appears. \n",
    "#4) --> feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#implementing n-grams Model\n",
    "import nltk\n",
    "    \n",
    "def generate_n_grams(data_lst, n):\n",
    "    cleaned = remove_chars(list(data_lst))\n",
    "    cleaned = clean_cols(cleaned)\n",
    "    cleaned = remove_stop_words(cleaned)\n",
    "    #make sure that n_grams 'refresh' when a new dataset is encountered!!!!   \n",
    "    return list(ngrams(cleaned, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating a n-gram frequency table (this is hopefully useful for determining if adjacent columns have effect on tags)\n",
    "#this can also be applied to exploring correlation between tags and attributes. \n",
    "#any applications to the data itself? Should I treat all of the data as a single list of words? Example:\n",
    "#currently thinking of using BOW to the data itself? \n",
    "\n",
    "def count_stats_grams(two_d_arr):\n",
    "    #np.unique 'axis' attribute doesn't work on my computer... \n",
    "    lst = np.array([])\n",
    "    count = 0\n",
    "    singles_count = 0\n",
    "    multiples_count = 0\n",
    "    for arr in two_d_arr:\n",
    "        if arr not in lst:\n",
    "            count += 1\n",
    "            np.append(lst, arr)\n",
    "        if two_d_arr.count(arr) == 1:\n",
    "            singles_count += 1\n",
    "        if two_d_arr.count(arr) > 1:\n",
    "            multiples_count += 1\n",
    "    check = count - singles_count\n",
    "    assert(check == multiples_count)\n",
    "    return count, singles_count, multiples_count\n",
    "\n",
    "def n_gram_freqs(dataframe, max_n = 4):\n",
    "    n_gram_cols = ['n-gram', 'data' ,'unique ngrams', 'multiples', 'singles']\n",
    "    n_gram_freqs = pd.DataFrame(columns = n_gram_cols)\n",
    "    for i in range(max_n):\n",
    "        n = i+1\n",
    "        n_grams = generate_n_grams(dataframe['Header'], n)\n",
    "        unique_n_grams, singles, multiples = count_stats_grams(n_grams)\n",
    "        row = {'n-gram': n, \n",
    "              'data': n_grams,\n",
    "              'unique ngrams': unique_n_grams,\n",
    "              'multiples': multiples,\n",
    "              'singles': singles}\n",
    "        n_gram_freqs.loc[len(n_gram_freqs)] = row\n",
    "    return pd.DataFrame(n_gram_freqs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Takes a data row and cleans it for model input\n",
    "import ast\n",
    "import itertools \n",
    "\n",
    "def word_extract(row):\n",
    "    ignore = ['nan']\n",
    "    #words = ast.literal_eval(row)\n",
    "    no_white = [i.lstrip() for i in row if i not in ignore and not isinstance(i, float)]\n",
    "    #divide_space = [i.split() for i in no_white]\n",
    "    cleaned_text = [w.lower() for w in no_white if w not in ignore]\n",
    "    return cleaned_text\n",
    "\n",
    "long_string = []\n",
    "for i in headers_and_tags['Data']:\n",
    "    result_by_tag = word_extract(i)\n",
    "    holder_list = ' '.join(result_by_tag)\n",
    "    long_string.append(holder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = long_string\n",
    "X_vecs = vectorizer.fit_transform(corpus)\n",
    "#np.set_printoptions(threshold = np.inf)\n",
    "#print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(X_vecs.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testing MLP Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from fastText import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fasttext_model = 'wiki.en.bin'\n",
    "fmodel = load_model(fasttext_model)\n",
    "print(\"Pre-trained model loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Classification using only headers\n",
    "df = headers_and_tags\n",
    "df['Header_embedding'] = df['Header'].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "df['Organization_embedded'] = df['Organization'].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "#df['data_embedding'] = df['Data'].map(lambda lst: [fmodel.get_sentence_vector(str(x)) for x in lst])\n",
    "#df['ngram']\n",
    "print(\"Word embeddings extracted!\\n\")\n",
    "\n",
    "#test = df['Header_embedding'] + df['Organization_embedded']\n",
    "#combining so that the matrix will be in the right shape...\n",
    "#not sure if this will make sense... \n",
    "#print(np.shape(test))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Header_embedding'], \n",
    "                                                    df['Tag'], test_size=0.33, random_state=0)\n",
    "\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "\n",
    "#have to ensure X_train is the right shape\n",
    "#temp = X_train.to_csv('temp.csv', header = False)\n",
    "#X_train = pd.read_csv('temp.csv', header = None)\n",
    "#os.remove('temp.csv')\n",
    "#X_train.index = X_train['0']\n",
    "#X_train.columns = [x for x in range(len(X_train.columns))]\n",
    "#X_train = np.reshape(X_train, (len(X_train), len(X_train.columns)))\n",
    "clf.fit(X_train.values.tolist(), y_train.values.tolist())\n",
    "test_score = clf.score(X_test.tolist(), y_test.tolist())\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classification using organization\n",
    "df = headers_and_tags\n",
    "df['Header_embedding'] = df['Header'].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "df['Organization_embedded'] = df['Organization'].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "#df['data_embedding'] = df['Data'].map(lambda lst: [fmodel.get_sentence_vector(str(x)) for x in lst])\n",
    "#df['ngram']\n",
    "print(\"Word embeddings extracted!\\n\")\n",
    "\n",
    "#test = df['Header_embedding'] + df['Organization_embedded']\n",
    "#combining so that the matrix will be in the right shape...\n",
    "#not sure if this will make sense... \n",
    "#print(np.shape(test))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Organization_embedded'], \n",
    "                                                    df['Tag'], test_size=0.33, random_state=0)\n",
    "\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "\n",
    "#have to ensure X_train is the right shape\n",
    "#temp = X_train.to_csv('temp.csv', header = False)\n",
    "#X_train = pd.read_csv('temp.csv', header = None)\n",
    "#os.remove('temp.csv')\n",
    "#X_train.index = X_train['0']\n",
    "#X_train.columns = [x for x in range(len(X_train.columns))]\n",
    "#X_train = np.reshape(X_train, (len(X_train), len(X_train.columns)))\n",
    "clf.fit(X_train.values.tolist(), y_train.values.tolist())\n",
    "test_score = clf.score(X_test.tolist(), y_test.tolist())\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ngrams = generate_n_grams(headers_and_tags['Header'], 3)\n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "X_vec_grams = vectorizer.fit_transform(ngrams)\n",
    "print(np.shape(X_vec_grams.toarray()))\n",
    "print(np.shape(X_vecs.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testing MLP Classifier on BOW \n",
    "df_2 = headers_and_tags\n",
    "df_2['BOW_counts'] = X_vecs.toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vecs.toarray(), \n",
    "                                                    df['Tag'], test_size=0.33, random_state=0)\n",
    "\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "\n",
    "#have to ensure X_train is the right shape\n",
    "#temp = X_train.to_csv('temp.csv', header = False)\n",
    "#X_train = pd.read_csv('temp.csv', header = None)\n",
    "#os.remove('temp.csv')\n",
    "#X_train.index = X_train['0']\n",
    "#X_train.columns = [x for x in range(len(X_train.columns))]\n",
    "#X_train = np.reshape(X_train, (len(X_train), len(X_train.columns)))\n",
    "clf.fit(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testing MLP Classifier on ngrams\n",
    "df_3 = headers_and_tags\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec_grams.toarray(), \n",
    "                                                    df['Tag'][0:len(X_vec_grams.toarray())], test_size=0.33, random_state=0)\n",
    "\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "\n",
    "#have to ensure X_train is the right shape\n",
    "#temp = X_train.to_csv('temp.csv', header = False)\n",
    "#X_train = pd.read_csv('temp.csv', header = None)\n",
    "#os.remove('temp.csv')\n",
    "#X_train.index = X_train['0']\n",
    "#X_train.columns = [x for x in range(len(X_train.columns))]\n",
    "#X_train = np.reshape(X_train, (len(X_train), len(X_train.columns)))\n",
    "clf.fit(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transforming data into right form\n",
    "\n",
    "df_target = headers_and_tags\n",
    "df_target['BOW_counts'] = [item for item in X_vecs.toarray()]\n",
    "n = len(X_vec_grams.toarray())\n",
    "df_target = df.iloc[0:n, :]\n",
    "df_target['ngrams_counts'] = [item for item in X_vec_grams.toarray()]\n",
    "df_target = df_target[['Header_embedding', \n",
    "                      'Organization_embedded',\n",
    "                      'BOW_counts',\n",
    "                      'ngrams_counts']]\n",
    "df_target.head()\n",
    "#print(X_vecs.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using VotingClassifier and Pipeline to combine all features to predict the 'most voted' output \n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "# custom transformer for sklearn pipeline\n",
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def transform(self, X):\n",
    "        col_list = []\n",
    "        for c in self.cols:\n",
    "            col_list.append(X[:, c:c+1])\n",
    "        return np.concatenate(col_list, axis=1)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_target, df['Tag']\n",
    "                                                   [0:len(X_vec_grams.toarray())])\n",
    "\n",
    "header_pipe = Pipeline([\n",
    "        ('col_extract', ColumnExtractor(cols = 1)), \n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "org_pipe = Pipeline([\n",
    "        ('col_extract', ColumnExtractor(cols = 2)), \n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "BOW_pipe = Pipeline([\n",
    "        ('col_extract', ColumnExtractor(cols = 3)), \n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "Ngram_pipe = Pipeline([\n",
    "        ('col_extract', ColumnExtractor(cols = 4)), \n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "eclf = VotingClassifier(estimators = [('p1', header_pipe),\n",
    "                                     ('p2', org_pipe),\n",
    "                                     ('p3', BOW_pipe),\n",
    "                                     ('p4', Ngram_pipe)], \n",
    "                                   voting = 'soft', \n",
    "                                   weights = [1, 0.5, 0.5, 0.5])\n",
    "eclf.fit(X_train, y_train)\n",
    "eclf_score = eclf.score(X_test, y_test)\n",
    "print(\"Classification accuracy on test set: %s\" %eclf_score)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
