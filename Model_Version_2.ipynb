{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIREMENTS TO RUN THE PYTHON DOCUMENT\n",
    "#1) hdx python api\n",
    "#2) pip install nltk\n",
    "\n",
    "create_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdx.utilities.easy_logging import setup_logging\n",
    "from hdx.hdx_configuration import Configuration\n",
    "from hdx.data.dataset import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import itertools \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from fastText import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logging()\n",
    "Configuration.create(hdx_site='prod', user_agent='A_Quick_Example', hdx_read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the dataset has at least 1 resource of the required file type(s).\n",
    "\n",
    "def check_type(dataset, file_types=[]):\n",
    "    temp_dataset = Dataset.read_from_hdx(dataset)\n",
    "    temp_dataset.separate_resources()\n",
    "    if (len(temp_dataset.resources) > 0):\n",
    "        if (len(file_types) > 0):\n",
    "            if (not set(temp_dataset.get_filetypes()).isdisjoint(file_types)): \n",
    "                    return True\n",
    "        else :\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset is tagged with HXL tag, not provided by HXL\n",
    "\n",
    "def check_organization(dataset):\n",
    "    if dataset.get_organization()['title'] != 'Humanitarian Exchange Language(HXL)':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "#CLEANING AND GENERATING N-GRAMS\n",
    "\n",
    "def lower_cols(lst):\n",
    "    #convert data to lowercases\n",
    "    #QUESTION: will I miss anyt important information? \n",
    "    return [word.lower() for word in lst]\n",
    "\n",
    "#Question: is HXL Core Schema.csv something we can use for comparing words??\n",
    "#This method is going to take up a lot of space and time. Is it worth it? Are there any other ways to go about it? \n",
    "\n",
    "def remove_chars(lst):\n",
    "    #remove punctuation characters such as \",\", \"(\", \")\", \"\"\", \":\", \"/\", and \".\"\n",
    "    #NOTE: PRESERVES WHITE SPACE.\n",
    "    #QUESTION: any other characters we should be aware of? Is this a good idea? I'm inspecting each word individually.\n",
    "    #Any potential pitfalls? \n",
    "    cleaned = [re.sub('\\s+', ' ', mystring).strip() for mystring in lst]\n",
    "    cleaned = [re.sub(r'[[^A-Za-z0-9\\s]+]', ' ', mystr) for mystr in cleaned]\n",
    "    cleaned = [mystr.replace('_', ' ') for mystr in cleaned]\n",
    "    return cleaned\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(data_lst):\n",
    "    #remove stopwords from the data including 'the', 'and' etc. \n",
    "    wordsFiltered = []\n",
    "    for w in data_lst:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered\n",
    "\n",
    "def clean_cols(data):\n",
    "    data = lower_cols(data)\n",
    "    data = remove_chars(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download one dataset with certain type(s), read it into Dataframe, \n",
    "# add all headers, tags and dataset names to our DataFrame,\n",
    "# and delete the dataset\n",
    "\n",
    "def process_dataset(dataset, file_type, dataframe, download_path, index, row_limit = 10):\n",
    "    global count\n",
    "    organization = \"\"\n",
    "# Download one dataset and read it into a DataFrame \n",
    "    if (file_type == None):\n",
    "        url, path = dataset.resources[0].download(download_path)\n",
    "        pandas_dataset = pd.read_csv(path)\n",
    "    else:\n",
    "        if (file_type not in dataset.get_filetypes()):\n",
    "            return 'Error: Required file type not in dataset OR dataset does not contain any resources.'\n",
    "        try:\n",
    "            url, path = dataset.resources[dataset.get_filetypes().index(file_type)].download(download_path)\n",
    "            organization = dataset.get_organization()['title']\n",
    "            print('Resource URL %s downloaded to %s' % (url, path))\n",
    "            pandas_dataset = pd.read_csv(path, encoding='latin-1')\n",
    "            pandas_dataset = pandas_dataset.head(row_limit)\n",
    "        except:\n",
    "            return 'Unknown error.'\n",
    "     \n",
    "    #if \"HXL\" in os.path.basename(path) or \"hxl\" in os.path.basename(path):\n",
    "        #return dataset_df\n",
    "    \n",
    "    # Add headers, tags and data to our DataFrame if current dataset not empty\n",
    "        if (not pandas_dataset.empty):\n",
    "            dataset_df = pandas_dataset\n",
    "            headers = list(dataset_df.columns.values)\n",
    "            headers = clean_cols(headers)\n",
    "            tags = list(dataset_df.iloc[0,:])\n",
    "            for i in range(len(headers)):\n",
    "                try:\n",
    "                    splitted = re.split('[(^\\s+)+#]', tags[i])\n",
    "                    splitted = list(filter(None, splitted))\n",
    "                    hashtag = splitted[0]\n",
    "                    attributes = splitted[1:]\n",
    "                    dic = {'Header': headers[i], 'Tag': hashtag, 'Attributes': attributes, \n",
    "                           'Data': list(dataset_df.iloc[1:, i]), \n",
    "                           'Relative Column Position': (i+1) / len(dataset_df.columns), \n",
    "                           'Dataset_name': os.path.basename(path), \n",
    "                           'Organization': organization,\n",
    "                           'Index': index}\n",
    "                    dataframe.loc[len(dataframe)] = dic\n",
    "                except:\n",
    "                    print(\"Error: different number of headers and tags\")\n",
    "            count += 1\n",
    "        os.remove(path)\n",
    "        print(\"File Removed!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for all datasets with HXL tags\n",
    "datasets_HXL = Dataset.search_in_hdx('HXL')\n",
    "len(datasets_HXL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for all headers and tags\n",
    "\n",
    "col_names = ['Header', 'Tag', 'Attributes','Data','Relative Column Position','Dataset_name', 'Organization','Index']\n",
    "headers_and_tags= pd.DataFrame(columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in n tagged datasets from HDX\n",
    "count = 0\n",
    "n = 150 #NUMBER OF DATASETS\n",
    "if (create_dataset):\n",
    "    for i in range(n):\n",
    "        rand_dataset = np.random.randint(0, len(datasets_HXL))\n",
    "        process_dataset_2(datasets_HXL[rand_dataset], 'CSV', headers_and_tags, './datasets', count)\n",
    "        print(i)\n",
    "        \n",
    "    headers_and_tags.to_excel(\"headerandtag.xlsx\")\n",
    "else:\n",
    "    headers_and_tags = pd.read_excel(\"headerandtag.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads an excel file with the above attributes\n",
    "headers_and_tags.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing n-grams Model\n",
    "\n",
    "def generate_n_grams(data_lst, n):\n",
    "    cleaned = remove_chars(list(data_lst))\n",
    "    cleaned = clean_cols(cleaned)\n",
    "    cleaned = remove_stop_words(cleaned)\n",
    "    #make sure that n_grams 'refresh' when a new dataset is encountered!!!!   \n",
    "    return list(ngrams(cleaned, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a n-gram frequency table \n",
    "\n",
    "def count_stats_grams(two_d_arr):\n",
    "    #np.unique 'axis' attribute doesn't work on my computer... \n",
    "    lst = np.array([])\n",
    "    count = 0\n",
    "    singles_count = 0\n",
    "    multiples_count = 0\n",
    "    for arr in two_d_arr:\n",
    "        if arr not in lst:\n",
    "            count += 1\n",
    "            np.append(lst, arr)\n",
    "        if two_d_arr.count(arr) == 1:\n",
    "            singles_count += 1\n",
    "        if two_d_arr.count(arr) > 1:\n",
    "            multiples_count += 1\n",
    "    check = count - singles_count\n",
    "    assert(check == multiples_count)\n",
    "    return count, singles_count, multiples_count\n",
    "\n",
    "def n_gram_freqs(dataframe, max_n = 4):\n",
    "    n_gram_cols = ['n-gram', 'data' ,'unique ngrams', 'multiples', 'singles']\n",
    "    n_gram_freqs = pd.DataFrame(columns = n_gram_cols)\n",
    "    for i in range(max_n):\n",
    "        n = i+1\n",
    "        n_grams = generate_n_grams(dataframe['Header'], n)\n",
    "        unique_n_grams, singles, multiples = count_stats_grams(n_grams)\n",
    "        row = {'n-gram': n, \n",
    "              'data': n_grams,\n",
    "              'unique ngrams': unique_n_grams,\n",
    "              'multiples': multiples,\n",
    "              'singles': singles}\n",
    "        n_gram_freqs.loc[len(n_gram_freqs)] = row\n",
    "    return pd.DataFrame(n_gram_freqs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a data row and cleans it for model input\n",
    "def word_extract(row):\n",
    "    ignore = ['nan']\n",
    "    no_white = [i.lstrip() for i in row if i not in ignore and not isinstance(i, float)]\n",
    "    cleaned_text = [w.lower() for w in no_white if w not in ignore]\n",
    "    return cleaned_text\n",
    "\n",
    "long_string = []\n",
    "for i in headers_and_tags['Data']:\n",
    "    result_by_tag = word_extract(i)\n",
    "    holder_list = ''.join(result_by_tag)\n",
    "    long_string.append(holder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = long_string\n",
    "X_vecs = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = 'wiki.en.bin'\n",
    "fmodel = load_model(fasttext_model)\n",
    "print(\"Pre-trained model loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification accuracy using only headers\n",
    "df = headers_and_tags\n",
    "df['Header_embedding'] = df['Header'].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "print(\"Word embeddings extracted!\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Header_embedding'], \n",
    "                                                    df['Tag'], test_size=0.33, random_state=0)\n",
    "\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "\n",
    "clf.fit(X_train.values.tolist(), y_train.values.tolist())\n",
    "test_score = clf.score(X_test.tolist(), y_test.tolist())\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification accuracy using organization\n",
    "df = headers_and_tags\n",
    "df['Organization_embedded'] = df['Organization'].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "print(\"Word embeddings extracted!\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Organization_embedded'], \n",
    "                                                    df['Tag'], test_size=0.33, random_state=0)\n",
    "\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "\n",
    "clf.fit(X_train.values.tolist(), y_train.values.tolist())\n",
    "test_score = clf.score(X_test.tolist(), y_test.tolist())\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing n-grams\n",
    "ngrams = generate_n_grams(headers_and_tags['Header'], 3)\n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "X_vec_grams = vectorizer.fit_transform(ngrams)\n",
    "print(np.shape(X_vec_grams.toarray()))\n",
    "print(np.shape(X_vecs.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing accuracy of MLP Classifier on BOW \n",
    "df_2 = headers_and_tags\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vecs.toarray(), \n",
    "                                                    df['Tag'], test_size=0.33, random_state=0)\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing accuracy of MLP Classifier on ngrams\n",
    "df_3 = headers_and_tags\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec_grams.toarray(), \n",
    "                                                    df['Tag'][0:len(X_vec_grams.toarray())], test_size=0.33, random_state=0)\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating embedded features into a single Dataframe\n",
    "\n",
    "df_target = headers_and_tags\n",
    "df_target['BOW_counts'] = [item for item in X_vecs.toarray()]\n",
    "n = len(X_vec_grams.toarray())\n",
    "df_target = df.iloc[0:n, :]\n",
    "df_target['ngrams_counts'] = [item for item in X_vec_grams.toarray()]\n",
    "df_target = df_target[['Header_embedding', \n",
    "                      'Organization_embedded',\n",
    "                      'BOW_counts',\n",
    "                      'ngrams_counts']]\n",
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = df_target.apply(lambda x: np.append(np.array([]), x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the hashtags on the predicted tags \n",
    "#add the pickle.dump \n",
    "#test nearest-neighbors\n",
    "#test randomforests "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
