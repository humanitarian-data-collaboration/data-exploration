{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hdx.utilities.easy_logging import setup_logging\n",
    "from hdx.hdx_configuration import Configuration\n",
    "from hdx.data.dataset import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Configuration.create(hdx_site='prod', user_agent='A_Quick_Example', hdx_read_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = Dataset.get_all_dataset_names()\n",
    "print(len(datasets)) #Total number of datasets obtained using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning all the selected datasets\n",
    "def clean(datasets, file_types=[]):\n",
    "    result = []\n",
    "    index = 0\n",
    "    while (index < len(datasets)):\n",
    "        temp_dataset = Dataset.read_from_hdx(datasets[index])\n",
    "        temp_dataset.separate_resources() #In order to use dataset.resources\n",
    "        if (len(temp_dataset.resources) > 0):\n",
    "            if (len(file_types) > 0):\n",
    "                if (not set(temp_dataset.get_filetypes()).isdisjoint(file_types)): #To check if elements of the 2 lists of file_types overlap.\n",
    "                    result.append(datasets[index])\n",
    "            else :\n",
    "                result.append(datasets[index])\n",
    "        index += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dload_dset(dataset_1,dload_path,row_limit=10,file_type='CSV') :\n",
    "    if (file_type == None):\n",
    "        url, path = dataset_1.resources[0].download(dload_path)\n",
    "        pandas_dataset = pd.read_csv(path)\n",
    "    else :\n",
    "        if (file_type not in dataset_1.get_filetypes()):\n",
    "            url = '1'\n",
    "            path = '2'\n",
    "            pandas_dataset ='9'\n",
    "            return 'Error: Required file type not in dataset OR dataset does not contain any resources.'\n",
    "        else :\n",
    "            url, path = dataset_1.resources[dataset_1.get_filetypes().index(file_type)].download(dload_path)\n",
    "            try:\n",
    "                pandas_dataset = pd.read_csv(path, encoding='latin-1')\n",
    "                os.remove(path)\n",
    "                pandas_dataset = pandas_dataset.head(row_limit)\n",
    "            except:\n",
    "                pandas_dataset = pd.DataFrame()\n",
    "        return pandas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hdx_dset_scraper(dload_path,limit=1000000000,download=True,file_type='CSV'):\n",
    "    datasets = Dataset.get_all_dataset_names()\n",
    "    if (limit > len(datasets)):\n",
    "        limit = len(datasets)\n",
    "    dset_dict = {}\n",
    "    index = 0\n",
    "    while (limit > index) :\n",
    "        dataset_1 = Dataset.read_from_hdx(datasets[index]) \n",
    "        dataset_1.separate_resources()\n",
    "        if (file_type not in dataset_1.get_filetypes()) :\n",
    "            index += 1\n",
    "            limit += 1\n",
    "        else :\n",
    "            dset_dict.update({datasets[index] : dload_dset(dataset_1,dload_path)})\n",
    "            index += 1\n",
    "    return dset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_10 = hdx_dset_scraper('Datasets',limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#storing all data into a dataframe\n",
    "df = pd.DataFrame.from_dict(data = dict_10, orient = 'index')\n",
    "\n",
    "#storing all the column names\n",
    "list(df.iloc[0,0].columns.values)\n",
    "col_names = [list(df.iloc[i,0].columns.values) for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
